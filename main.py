import streamlit as st # pip install streamlit
import openai # pip install openai
import pandas as pd # pip install pandas
import os
from dotenv import load_dotenv # pip install python-dotenv

load_dotenv()

# OpenAI API ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
openai.azure_endpoint = os.getenv("AZURE_ENDPOINT")
openai.api_type = os.getenv("OPENAI_API_TYPE")
openai.api_version = os.getenv("OPENAI_API_VERSION")

st.set_page_config(page_title="AI ê°ì • ì½”ì¹˜", layout="wide")

# -------------------------------
# í™”ë©´ ì „í™˜ ìƒíƒœ ê´€ë¦¬
# -------------------------------
if "page" not in st.session_state:
    st.session_state.page = "main"

# -------------------------------
# ë©”ì¸ í˜ì´ì§€
# -------------------------------
if st.session_state.page == "main":
    st.title("ğŸ§  AI ê°ì • ì½”ì¹˜")
    st.subheader("ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ê°ì •ì€ ì–´ë–¤ê°€ìš”?")

    emotion = st.radio("ê°ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš”:", ["ğŸ˜Š í–‰ë³µ", "ğŸ˜ í‰ì˜¨", "ğŸ˜¢ ìŠ¬í””", "ğŸ˜  ë¶„ë…¸", "ğŸ˜© í”¼ë¡œ", "ğŸ˜¨ ë¶ˆì•ˆ"])

    if st.button("ì½”ì¹­ ì‹œì‘í•˜ê¸°"):
        st.session_state["emotion"] = emotion
        st.session_state["page"] = "chat"
        st.stop()

# -------------------------------
# ì±—ë´‡ í˜ì´ì§€
# -------------------------------
elif st.session_state.page == "chat":
    selected_emotion = st.session_state.get("emotion", None)

    if not selected_emotion:
        st.warning("ê°ì •ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì¸ í™”ë©´ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
        st.session_state.page = "main"
        st.experimental_rerun()

    st.info(f"ğŸ§  ì„ íƒí•œ ê°ì •: **{selected_emotion}**")
    st.title("ğŸ’¬ ê°ì • ì½”ì¹­ ì±—ë´‡")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    user_input = st.chat_input("ì§€ê¸ˆ ì–´ë–¤ ê¸°ë¶„ì´ì‹ ê°€ìš”?")

    if user_input:
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        system_prompt = f"ë‹¹ì‹ ì€ ê°ì • ì½”ì¹˜ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í˜„ì¬ ê°ì •ì€ '{selected_emotion}'ì…ë‹ˆë‹¤. ì´ì— ë§ì¶° ê³µê°í•˜ê³  ì½”ì¹­í•˜ì„¸ìš”."
        messages = [{"role": "system", "content": system_prompt}] + st.session_state.chat_history
        response = openai.chat.completions.create(
            model="dev-gpt-4.1-mini",
            messages=messages,
            temperature=0.7
        )
        reply = response.choices[0].message.content
        st.session_state.chat_history.append({"role": "assistant", "content": reply})

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # -------------------------------
    # ì²¨ë¶€ ì˜ì—­
    # -------------------------------
    with st.expander("ğŸ“ íŒŒì¼ ë° URL ì²¨ë¶€"):
        uploaded_excel = st.file_uploader("ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"])
        uploaded_video = st.file_uploader("ë™ì˜ìƒ ì—…ë¡œë“œ", type=["mp4", "mov"])
        url_input = st.text_input("ğŸ”— URL ì…ë ¥ (ë©”ì¼, ë¬¸ì„œ ë“±)")

    text = ""
    sources = []
    if uploaded_excel:
        st.session_state.chat_history.append({"role": "user", "content": uploaded_excel.name})

        df = pd.read_excel(uploaded_excel)
        sources.append("ì—‘ì…€")
        text = " ".join(df.astype(str).fillna("").values.flatten())

        prompt = f"ì‚¬ìš©ìì˜ í˜„ì¬ ê°ì •ì€ '{selected_emotion}'ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•˜ê³ , ê°ì • ì½”ì¹­ì„ í•´ì¤˜: {text}"

        with st.spinner("ğŸ§  ê°ì • ì½”ì¹­ ë¶„ì„ ì¤‘..."):
            prompt = f"ì‚¬ìš©ìì˜ í˜„ì¬ ê°ì •ì€ '{selected_emotion}'ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•˜ê³ , ê°ì • ì½”ì¹­ì„ í•´ì¤˜: {text}"
            response = openai.chat.completions.create(
                model="dev-gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )


            reply = response.choices[0].message.content

            # ëŒ€í™” íë¦„ì— ì¶”ê°€
            st.session_state.chat_history.append({"role": "assistant", "content": reply})

            # í™”ë©´ì— í‘œì‹œ
            with st.chat_message("assistant"):
                st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
                st.markdown("ğŸ§  AI ì½”ë©˜íŠ¸:")
                st.markdown(reply)

            uploaded_excel = None  # ì—…ë¡œë“œ ì´ˆê¸°í™”

    if uploaded_video:
        st.session_state.chat_history.append({"role": "user", "content": uploaded_video.name})
        # ë™ì˜ìƒ ì–¼êµ´ì¸ì‹
        import cv2 # pip install opencv-python
        from deepface import DeepFace # pip install deepface

        video_path = uploaded_video.name
        cap = cv2.VideoCapture(video_path)
        
        frame_count = 0
        emotions = []

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret or frame_count > 100:  # 100í”„ë ˆì„ê¹Œì§€ë§Œ ë¶„ì„
                break

            try:
                result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
                emotions.append(result[0]['dominant_emotion'])
            except:
                pass

            frame_count += 1

        cap.release()

        # ë™ì˜ìƒ ìŒì„±ì¸ì‹
        import whisper # pip install openai-whisper

        # Whisper ëª¨ë¸ ë¡œë”©
        model = whisper.load_model("base")

        # ë™ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (ffmpeg í•„ìš”)
        import subprocess # pip install ffmpeg-python
        audio_path = "audio.wav"
        subprocess.run(["ffmpeg", "-i", uploaded_video.name, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path])

        # ìŒì„± ì¸ì‹
        result = model.transcribe(audio_path)
        transcript = result["text"]

        # ê°ì • ìš”ì•½
        from collections import Counter 
        emotion_summary = Counter(emotions).most_common()

        # ì–¼êµ´ ê°ì • ë¶„ì„

        # ì–¼êµ´ ê°ì • ë¶„ì„ ê²°ê³¼ ìš”ì•½
        text += f"ì˜ìƒì—ì„œ ì¶”ì •ëœ ê°ì •: {emotion_summary}\n"
        sources.append("ë™ì˜ìƒ(ì–¼êµ´)")

        # ìŒì„± ì¸ì‹ ê²°ê³¼
        text += f"ì‚¬ìš©ìì˜ ìŒì„± ë‚´ìš©: {transcript}\n"
        sources.append("ë™ì˜ìƒ(ìŒì„±)")

        # GPTì—ê²Œ ì „ë‹¬
        prompt = f"ì‚¬ìš©ìì˜ í˜„ì¬ ê°ì •ì€ '{selected_emotion}'ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ë¶„ì„í•˜ê³ , ê°ì • ì½”ì¹­ì„ í•´ì¤˜: {text}"
        response = openai.chat.completions.create(
            model="dev-gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        st.markdown(f"ğŸ“ ë¶„ì„ ëŒ€ìƒ: {', '.join(sources)}")
        st.markdown("ğŸ§  AI ì½”ë©˜íŠ¸:")
        st.markdown(response.choices[0].message.content)

        uploaded_video = None  # ì—…ë¡œë“œ ì´ˆê¸°í™”
    if url_input:
        st.session_state.chat_history.append({"role": "user", "content": url_input})

        import requests # pip install requests  
        from bs4 import BeautifulSoup # pip install beautifulsoup4  
        
        try:
            # 1. URL ì ‘ì† ë° HTML ê°€ì ¸ì˜¤ê¸°
            with st.spinner("ğŸ§  ê°ì • ì½”ì¹­ ë¶„ì„ ì¤‘..."):
                response = requests.get(url_input)
                soup = BeautifulSoup(response.text, "html.parser")

                # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë©”ì¼ ë‚´ìš©ìœ¼ë¡œ ê°€ì •)
                # ì‹¤ì œ ë©”ì¼ êµ¬ì¡°ì— ë”°ë¼ íƒœê·¸ ì¡°ì • í•„ìš”
                body_text = soup.get_text(separator=" ", strip=True)

                # 3. GPTì—ê²Œ ê°ì • ë¶„ì„ ìš”ì²­
                prompt = f"ë‹¤ìŒ ë©”ì¼ ë‚´ìš©ì„ ì½ê³ , ë³´ë‚¸ ì‚¬ëŒì˜ ê°ì •ì„ ë¶„ì„í•´ì¤˜:\n\n{body_text}"
                response = openai.chat.completions.create(
                    model="dev-gpt-4.1-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7
                )
                reply = response.choices[0].message.content

                # ëŒ€í™” íë¦„ì— ì¶”ê°€
                st.session_state.chat_history.append({"role": "assistant", "content": reply})

                # í™”ë©´ì— í‘œì‹œ
                with st.chat_message("assistant"):
                    st.markdown(reply)
        except Exception as e:
            st.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        url_input = ""  # ì…ë ¥ ì´ˆê¸°í™”